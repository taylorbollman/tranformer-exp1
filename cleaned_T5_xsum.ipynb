{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# import pandas as pd\n",
    "# from IPython.display import display, HTML\n",
    "\n",
    "import torch\n",
    "from torch import utils\n",
    "from torch.utils import data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "import accelerate\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# import huggingface_hub\n",
    "from transformers import T5ForConditionalGeneration, T5Config, T5TokenizerFast, T5Tokenizer, AutoTokenizer\n",
    "from transformers import DataCollatorWithPadding, DataCollatorForSeq2Seq\n",
    "from transformers import TrainingArguments, Trainer, default_data_collator, get_scheduler\n",
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset #, load_from_disk\n",
    "# import evaluate\n",
    "# from evaluate import load\n",
    "\n",
    "\n",
    "import tqdm as notebook_tqdm\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "huggingface_token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "# huggingface_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "device_type = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(device_type)\n",
    "# print(torch.cuda.device_count())\n",
    "cpu_cores = multiprocessing.cpu_count()\n",
    "# print(cpu_cores)\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.state.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"t5-small\"\n",
    "tokenizer = T5TokenizerFast.from_pretrained(model_checkpoint)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = load_dataset(\"xsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 1000\n",
    "test_size = int(0.1 * train_size)\n",
    "downsampled_dataset = raw_dataset[\"train\"].train_test_split(train_size=train_size, test_size=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "670f6f0408fb436180b3692163d4c886",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=7):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniforge3/envs/base_ggl/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf9050b689dd42fdac4f403382a3ab70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=7):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniforge3/envs/base_ggl/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "prefix = \"summarize: \"\n",
    "max_input_length = 512\n",
    "max_target_length = 128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"document\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    labels = tokenizer(text_target=examples[\"summary\"], max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = downsampled_dataset.map(preprocess_function, batched=True, num_proc=(cpu_cores-1))\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "test_dataset = tokenized_datasets[\"test\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base_ggl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
