# # !touch .env
# # !touch .gitignore
# # !echo '.env' >> .gitignore
# # !echo 'HUGGINGFACE_TOKEN="hf_ONAr"' >> .env'
# load_dotenv()
# huggingface_token = os.getenv("HUGGINGFACE_TOKEN")
# # print(huggingface_token)
# # huggingface_hub.login(token=huggingface_token, add_to_git_credential=True)

LlamaConfig {
  "attention_bias": false,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.35.2",
  "use_cache": true,
  "vocab_size": 32000
}


rope_scaling={"type": "dynamic", "factor": 2.0}



    # tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf", )
    # model = AutoModelForCausalLM.from_pretrained(
    #     model=model_name,
    #     torch_dtype=torch.float16,
    #     device_map="auto",
    #     rope_scaling={"type": "dynamic", "factor": 2.0},
#     huggingface_token=huggingface_token,
#     )


torchrun --nproc_per_node 2 ./nlp_example.py --mixed_

    # parser = argparse.ArgumentParser(description="Simple example of training script.")
    # parser.add_argument(
    #     "--mixed_precision",
    #     type=str,
    #     default="no",
    #     choices=["no", "fp16", "bf16", "fp8"],
    #     help="Whether to use mixed precision. Choose"
    #     "between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >= 1.10."
    #     "and an Nvidia Ampere GPU.",
    # )
    # parser.add_argument("--cpu", action="store_true", help="If passed, will train on the CPU.")
    # print('here ya go:', parser.parse_args())
    # args = parser.parse_args()
